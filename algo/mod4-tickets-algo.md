# Билеты к экзу по алгосне
## Модели вычислений
### <span style="color:green">(a)</span> 1. RAM и RAM-w, определения, различия.
* `RAM` - модель вычислений с произвольным доступом к памяти любой длины за $\mathcal{O}(1)$. Поддерживает операции:
    * `move` (копировать кусок памяти);
    * `add`, `sub`, `mul`, `div` (арифметические операции);
    * `jz` (переход к метке если `reg==0`);
    * `jump` (безусловный переход к метке);
    * `halt` (остановка выполнения программы);
    * `in` (ввод в `reg`);
    * `out` (вывод из `reg`).
* `RAM-w (word-RAM)` - модель вычислений, в которой мы программируем; `RAM` с ограничением "за $\mathcal{O}(1)$ операции происходят только с числами из $[0, 2^w)$; ячейки памяти $w$-битовые"; $w$-битовое число = word = машинное слово, операции с числами происходят за $\mathcal{O}(Time(n)/w)$, более грубая оценка: $n<2^w \Leftrightarrow \log n<w$, $\mathcal{O}(Time(n)/\log n)$.
### <span style="color:green">(a)</span> 2. RAM-w. Флойд за $\mathcal{O}(n^3/w)$ и $O(n^2)$ в RAM.
* `RAM-w Floyd`:
```python
for k=1..n # V^3/64
    for i=1..n
        if (d[i][k])
            for j=1..n
                d[i][j] |= d[k][j] # bitset
```
* `RAM Floyd`:
```python
for k=1..n
    for i=1..n
        if ((d[i] >> k) & 1) == 1 # O(1)
            d[i] |= d[k] # O(1)
```
### <span style="color:blue">(b)</span> 3. RAM-w. bfs за $\mathcal{O}(n^2/w)$ и $O(n)$ в RAM.
* `RAM-w bfs`:
```python
while queue.size(): # n steps
    v = queue.pop() # O(1)
    for u in g[v]: # n steps
        used[u] = True
        queue.push(u)
```
* `RAM bfs`:
```python
while queue.size(): # n steps
    v = queue.pop() # O(1)
    while g[v] & ((2**n - 1) - used) != 0: # O(1)
        x = lowerbit(g[v] & ((2**n - 1) - used)) # O(1)
        used |= 2**x # O(1)
        queue.push(x) # O(1)
```
### <span style="color:green">(a)</span> 4. RAM. P = NP. Обоснование, общий принцип.
* `Общий принцип`: можем параллелить проверку всех $2^{P(n)}$ подсказок $y$, используя массивы длины $w\cdot 2^{P(n)}$, где $w$ - битовый размер слова.
### <span style="color:blue">(b)</span> 5. RAM. Создание числа-массива из $n$ $w$-битных единиц.
* Пусть $E_0 = 0\dots01$ - $w$-битная единица. Тогда:
    * $E_{k+1} = (E_k << w2^k) + E_k$ - сдвинули все имеющиеся единицы влево, а вправо вернули то что было и получили такой же массив из $w$-битных единиц, но вдвое длиннее. 
### <span style="color:blue">(b)</span> 6. RAM. Сумма массива из $w$-битных чисел за $O(\log n)$.
* Дан массив из $n$ бит aka число $A$. Хотим логарифмический `bitcount`.
* Группируем суммы битов в кусочках длины $2^i$:
    * `i=1`: $y = (A \& 01\dots01) + ((A >> 1) \& 01\dots01)$ - четные оставляем, нечётные сдвигаем на 1 и потом складываем.
    * `i=2`: $z = (y \& 0011\dots0011) + ((y >> 2) \& 0011\dots 0011)$ - так же, но уже с шагом 2.
    * `i=4`: $r = (z \& 00001111\dots 00001111) + ((z >> 2) \& 00001111\dots00001111)$ - аналогично с шагом 4.
* На каждом шаге объединяем суммы двух соседних групп, размер группы удваивается, через $\log n$ шагов будет как раз одна сумма по всем $n$ битам.
* Каждый шаг - арифметические и битовые операции $\Rightarrow$ кол-во шагов = сложность = $\mathcal{O}(\log n)$ RAM-операций.
### <span style="color:red">(c)</span> 7. RAM. Конкретный алгоритм для 3-SAT в RAM за $\mathcal{O}(m + n)$.
* Нам нужно число $Y$, содержащее все $2^n$ подказок в одном $w2^n$-битном числе, а так же число $E = 2^n$, в каждом машинном слове единица. И то и то делаем за $\mathcal{O}(\log 2^n = n)$ последовательным удваиванием:
    * $Y_{k+1} = ((Y_k + E_k2^k) << w2^k) + Y_k$ - по сути удвоили каждую перестановку, сдвинули влево, а вправо вернули что было...
    * $E_{k+1} = (E<<w2^k)+E_k$ - просто сдвинули влево и повторили справа.
* Тогда решение выглядит как:
```python
result = E
for i=1..m: # проверяем каждый клоз
    F0 = ((Y >> a[i][0]) & E) ^ (e[i][0] ? E : 0)
    F1 = ((Y >> a[i][1]) & E) ^ (e[i][1] ? E : 0)
    F2 = ((Y >> a[i][2]) & E) ^ (e[i][2] ? E : 0)
    result &= F0 | F1 | F2
return result != 0 # хотя бы в одном y результат проверки 1.
```
* Получили построение $E$ и $Y$ за $O(n)$ и проверку за $O(m)$, в сумме $\mathcal{O}(n + m)$.
### <span style="color:red">(c)</span> 8. Машина Тьюринга. Определение.
* Коротко: максимально простой язык программирования; остальные называют Тьюринг-полными, т.е. умеют то же, что машина Тьюринга.
* Машина тьюринга состоит из:
    * `Бесконечная лента` $A$, в каждой ячейке символ алфавита $\Sigma$.
    * `Указатель` $p$ - целое число, указывает на текущую ячейку ленты, умеем двигать на +-1.
    * `Состояние` $S$ - множество всех возможных действий, $s$ - начальное состояние, $t$ - конечное (программа завершилась).
    * `Правила` $M$ - таблица переходов, по текущему состояню $s$ и символу $A_p$ решаем какой символ записать, в какое состояние перейти, сместить ли $p$, то есть:
        * $M:\langle s, A_p \rangle \to \langle s',c,p'\rangle$, т.ч. $|p-p'|=1$
* Универсальна: можно запрограммировать всё что угодно (хоть и сложно).
## Бинарные деревья поиска

### <span style="color:green">(a)</span> 9. BST. Add, Del, Find, LowerBound за $\mathcal{O}(h)$. Симметричный обход, сортировка, нижняя оценка.
* `BST (Binary Search Tree)` - структура данных, в вершине пара $\langle x, data \rangle$, в левом поддереве пары со строго меньшим $x$, в правом - со строго большим.
* Операции:
    * `Find`: спускаемся от корня вниз: если искомый меньше, идём налево, иначе направо.
    * `Add`: то же, что `Find`, либо найдём $x$ и ничего делать не надо, либо выйдем за пределы дерева и там вставим новое значение.
    * `Del`: нашли `x`, если 1 или 0 детей, просто удаляем а ребенка подвешиваем к ее родителю, иначе найдём следующий за $x$ элемент (1 раз вправо и до упора влево), свапнем их местами и удалим $x$.
    * `LowerBound`: идем от корня, если узел $\geqslant x$, он может быть ответом, запоминаем его как кандидата, идём влево; если $<x$, идём вправо, в какой-то момент найдём $x$ и вернём его, иначе выйдем из дерева и вернём кандидата.
* Симметричный обход: рекурсивно обойти левое поддерево (если есть), выписать $x$, рекурсивно обойти правое поддерево (если есть).
* Сортировка: просто вставить все элементы и сделать симметричный обход, выходит что вставка в среднем работает не лучше, чем за логарифм.
### <span style="color:green">(a)</span> 10. BST. Next, Prev, Find за O(1). Список. Хеш-таблица. Итератор в set.
* Ускорение операций:
    * `Find` за $\mathcal{O}(1)$: прикрутим хе-таблицу $x \to Node$, где $Node = \langle x, data \rangle$.
    * `Next/Prev` за $\mathcal{O}(1)$: прикрутим двусвязный список на вершинах (прошивка).
    * `Add/LowerBound` нельзя, так как через них выражается сортировка.
* У `std::set` под капотом BST (RB-tree), при модификации меняются только указатели между вершинами $\Rightarrow$ итераторы не инвалидируются, а ещё можем с помощью прошивки умеем быстро ++ и --.
### <span style="color:blue">(b)</span> 11. BST. Add/LowerBound оценка снизу $\Omega(\log n)$.
* С помощью этих функций можем выразить сортировку $\Rightarrow$ нельзя быстрее чем за $n \log n$.
### <span style="color:blue">(b)</span> 12. BST. Обработка равных ключей. Удаление за $\mathcal{O}(1)$.
* Несколько подходов для равных ключей:
    * Вместо $x_i$ ключом сделать пару $\langle x, i \rangle$.
    * В качестве $data$ хранить число вхождений/список элементов с таким ключом.
    * Ослабить определение и хранить в правом поддереве ноды с большими либо равными $x$. Что-то может сломаться.
    * Ещё сильнее ослабить определение и хранить равные ключи хоть слева хоть справа.
* `Del` за $\mathcal{O}(1)$: выражается через `Find` + `Next` + $\mathcal{O}(1)$.
### <span style="color:blue">(b)</span> 13. Персистентная версия добавления, удаления.
* Добавление:
```cpp
Node* add(Node* v, int x) {
    if (!v)
        return new Node(x);
    else if (x < v->x)
        return new Node(v->x, add(v->l, x), v->r);
    else
        return new Node(v->x, v->l, add(v->r, x));
}
```
* Удаление аналогично, но надо думать о свапе с `Next`.
### <span style="color:red">(c)</span> 14. BST. Прямой обход. Восстановление дерева за $\mathcal{O}(n)$.
* Алгоритм: идём по массиву, поддерживаем стек. Видим $x$, снимаем со стека всё, что $<x$. Делаем $x$ правым ребёнком последней снятой. Если ничего не сняли, делаем $x$ левым ребёнком вершины стека, кладём $x$ в стек.
### <span style="color:green">(a)</span> 15. ABL. Инвариант, типы вращений. Оценка глубины. Add.
* `AVL-инвариант` - в каждой вершине разность высот детей (высота - максимальное расстояние до листа в поддереве) не более, чем 1.
* `Глубина` AVL-дерева - $O(\log n)$.
    * `Д-во`: Пусть $S_h$ - минимальный размер дерева высоты $h$, тогда $n = S_h \geqslant S_{h-1} + S_{h - 2} \Rightarrow S_h \geqslant h$-го числа Фибоначчи, $h\leqslant \log n$. 
* `Вращения` хорошо показаны картинками в конспекте, описывать эту хуету словами не вижу смысла.
* `Add`: Обозначим за $h_0$ высоты до `add`, $h1$ - до вызова `rebalance`, $h2$ - после.\\
`Гарантии на вход`: $v$ - корень корректного AVL дерева, $|h_1(v.l)-h_1(v.r)|\leqslant 2$.\\
`Гарантии на выход`: Пусть `u = add(v, x)`. Тогда $u$ - корень корректного AVL-дерева, $h_0(v) \leqslant h_2(u) \leqslant h_0(v) + 1$.\\
`Доказательство`: НУО, пусть рекурсивный запуск вызван от левого поддерева. После этого оба корректные AVL-деревья, правое не менялось, левое по индукции. По итогу $h_0(v) \leqslant h_1(v) \leqslant h_0(v) + 1$. Теперь запускаем `u = rebalance(v)` (имеем право т.к. гарантия на вход $|h_0(v.l)-h_0(v.r)|\leqslant 1$ и одна из глубин увеличилась максимум на 1, разница высот не стала больше 2). Если AVL-условие уже выполнено, то `rebalance` ничего не сделает. Если оно перестало выполняться, то $h_1(v.l) - h_1(v.r) = 2$, $h_1(v) = h_0(v) + 1$. По гарантии `rebalance` на выход, $u$ - корень корректного AVL-дерева, тогда $h_1(v) - 1 \leqslant h_2(u) \leqslant h_1(v)$, $h_0(v)+1-1\leqslant h_2(u) \leqslant h_0(v) + 1$. ч.т.д.
### <span style="color:blue">(b)</span> 16. AVL. Del, Merge (без док-ва перебалансировки на $k$). Персистентное вращение.
* `Del`: Сначала сделаем `Del` как в обычном BST, потом пойдём по предкам и будем проводить ребалансировку. Высота могла измениться на 1 $\Rightarrow$ гарантии на вход ребалансировки соблюдены. После ребалансировки высота может не измениться, а может измениться на 1, тогда понадобится ребалансировка выше. В худшем случае $\Omega(\log n)$ вращений.
* `Merge`:
    * `Способ 1`: Удалим `L` из большего дерева за $\mathcal{O}(\log n)$. Сделаем его корнем, подвесим наши два дерева, перебалансируем за $\mathcal{O}(\log n)$ с помощью предыдущего пункта.
    * `Способ 2`:
```cpp
node* Merge(node* l, node* r) {
    if (!l || !r) return l ? l : r
    if (l->h <= r->h)
        r->l = Merge(l, r->l), Rebalance(r); return r;
    else
        l->r = Merge(l->r, r), Rebalance(l); return l;
}
```
* `Персистентные повороты`:
```cpp
// Node = {x, l, r}
Node* rotateRight(Node* v) {
    return new Node {v->l->x, v->l->l, new Node{v->x, v->l->r, v->r}};
}

Node* rotateLeft(Node *v) {
    return new Node {v->r->x, new Node{v->x, v->l, v->r->l}, v->r->r};
}

Node* roatateLeftRight(Node* v) {
    return rotateRight(new Node{v->x, rotateLeft(v->l), v->r})
}

Node* rotateRightLeft(Node* v) {
    return rotateLeft(new Node{v->x, v->l, rotateRight(v->r)});
}
```
### <span style="color:red">(c)</span> 17. AVL. Число вращений при Add и Del.
* На самом деле при добавлении происходит $\mathcal{O}(1)$ присваиваний указателей, т.е. очень мало вращений.
* При удалении в худшем случае $\Omega(\log n)$ вращений.
### <span style="color:red">(c)</span> 18. AVL. Split (с док-вом перебалансировки на k).
* `Split`: Пусть `t->x < x`, тогда рекурсивно делаем сплит `t->r`. Левую часть результата запишем обратно в `t->r` и сделаем `rebalance(t)`. Каждый `rebalance` работает за $\mathcal{O}(\log n) \Rightarrow$ время работы `Split` $\mathcal{O}(\log^2 n)$, хотя по факту только первый `rebalance` работает за логарифм, дальше оба дерева-результата имеют высоту, на $\mathcal{O}(1)$ отличающуюся от исходного и ребалансировка работает за единичку.
* `Утверждение`: при дисбалансе равном $k$ умеем rebalance за логарифм
    * `Доказательство`: Индукция по $k$. По индукции скажем, что для любого $i$ < k можем за $O(i)$ исправить дисбаланс, и глубина корня не изменится либо уменьшится на 1. Пусть у нашей вершины $d$ высота $h+1$, у левого ребенка $b$ $h$, у правого $e$ $h-k$. Дети левого ребёнка - $a$ и $c$. Малое вращение вправо и в любом случае получаем уменьшение дисбаланса на 1 или 2. см. 25 практику с.4 "Перебалансировка при дисбалансе $k$".
### <span style="color:green">(a)</span> 19. Общие идеи: BST и неявный ключ, BST и персистентность.
* `Неявный ключ`: не храним ключ. Храним размер поддерева и минимум в поддереве, и умеем делать `Find`, `Add`, `Del`, `Min`.
* `Персистентность`: вместо того, чтобы менять старые вершины, на обратном пути рекурсии создаём новые. Это `path-copying`, можно ещё `fat-nodes`. Научились делать персистентный массив, работающий за логарифм $\Rightarrow$ в любой СД вместо массивов используем персистентные BST по неявному ключу и получаем персистентность ценой замедления в $\mathcal{O}(\log n)$ раз...
### <span style="color:blue">(b)</span> 20. Общие идеи: запрос на отрезке BST-дерева, отложенные операции, разворот отрезка.
* `Запросы на отрезке`: каждой вершине BST с размером поддерева соответствует поддерево, являющееся списком значений. Умеем `getMin(l, r)` за $\mathcal{O}(\log n)$ спуском по дереву:
```cpp
int getMin(Node*v, int vl, int vr, int l, int r) {
    if (!v || r < vl || vr < l) return INT_MAX;
    if (l <= vl && vr <= r) return v->min_y;
    return min(
        getMin(v->l, vl, v->x - 1, l, r),
        getMin(v->r, v->x + 1, vr, l, r),
        (l <= v->x && v->x <= r) > v->y : INT_MAX
    );
}
```
* `Модификации на отрезке`: если хотим сразу, то будет линия. Используем отложенные операции. В общем в вершинке храним сколько нужно добавить/отнять, и делаем это только когда проходим через неё. Тогда `rangeInc` работает как и `getMin`, за логарифм.
* `Разворот отрезка`: высплитим отрезок $[l, r)$, отложим на нём необходимую операцию, а потом смёржим обратно.
### <span style="color:green">(a)</span> 21. B-дерево. Find, Add. Оценка $\mathcal{O}(k \log_k n)$ и $\mathcal{O}(\log_k n)$ чтений.
* `B-дерево`: зафиксируем число $k$. Вершина дерева - множество от $k-1$ до $2k-2$ ключей. Если в вершине хранится $i$ ключей, у нее $i+1$ ребёнок. При этом $T_1<x_1< T_2 < \dots < x_i < T_{i+1}$. BST - такое дерево с $k=2$. Глубина $B$-дерева = $\Theta(\log_k n)$.
* `Find`: при спуске вниз используем бинпоиск/поиск по BST за $\log k$, итого $\log_k n \cdot \log k = \log n$ процессорных операций. При этом обращений к диску всего $\log_k n$, так что не ухудшая времени минимизировали число обращений к диску.
* `Add`: Спустимся до листьев и добавим в пачку новый ключ. Если листьев стало слишком много, вершина "переполнилась" и делим её на 2 вершины по $k-1$-ому ключу, и его вставим в соответствующее место отцовской вершины. То бишь, средний ключ "всплыл" вверх на 1 уровень. Рекурсивно продолжаем толкать ключи вверх, получаем в отсортированном массиве вставку за $\mathcal{O}(k \log_k n)$, в дереве поиска за $\mathcal{O}(\log n)$.
### <span style="color:blue">(b)</span> 22. B-дерево. Find, Add, Del. Оценка $\mathcal{O}(\log n)$. Split, Merge.
* `Del`: Если не лист, свапаем с `Next`, удаляем из листа. Могло стать меньше `k-1` ключей. Тогда берём "брата" (соседнюю вершину с тем же отцом) и спускаем отца вниз, получили большую вершину, возможно слишком - тогда делим напополам, как в `Find`: По сути отобрали часть вершин у толстого брата. Ну и чиним возможно созданный нами дисбаланс идя вверх. Умеем за $\mathcal{O}(\log n)$, пользуясь логарифмическими `split/merge` для BST.
* `Merge`: Пусть $l$ выше, чем $r$. Тогда удаляем из $r$ минимальный (самый левый) ключ. Спускаемся в $l$ по правой ветви к вершине на нужной высоте (у которой высота = $h_r$), соединяем по ключу $x$ между их крайними детьми. Могла получиться слишком толстая вершина, тогда рекурсивно чиним как при добавлении.
* `Split`: Делим корень по $x$. Получились половины $L$ и $R$ (возможно пустые), между ними ребёнок $t$. рекурсивно делим $t$ на 2 половины, левая идёт сыном к $L$, правая к $R$. Если эти новодобавленные дети слишком малышарики, то делаем как в `Del`.
### <span style="color:blue">(b)</span> 23. RB-дерево. Определение.
* `RB-дерево`: (используется в `C++.STL` внутри `set`) BST, в котором каждая вершина покрашена в красный или чёрный. Также в каждое место "отсутствия сына" добавим фиктивного сына. Дерево красно-чёрное, если: 
    * На пути от корня до любой фиктивной вершины одинаковое количество чёрных вершин.
    * У красной вершины нет красных детей.
    * Корень - чёрная вершина.
### <span style="color:red">(c)</span> 24. B-дерево. Вариации. B*-дерево, B $^+$-дерево, 2-3-Tree, 2-3-4, RB, AA.
* `B*-дерево`: в каждой вершине $[k; 1.5k]$ ключей.
    * Вершина переполнилась? Перекидываем ключ в брата. Брат переполнился? Значит он ровно $1.5k$, у нас с ним в сумме $3k+1$ и ещё один соединительный в предке, делим на 3 по $k$ + 2 соединительных.
    * В вершине $k-1$ ключ? Пытаемся отцепить ключ от одного из ближайших братьев, не получилось? Братья ровно по $k$, в сумме $3k-1$ и 2 соединительных, мержимся на 2 по $1.5k$ + 1 соединительный. 
    * У корня братьев нет, ему можно $[1;2k]$ ключей. Если $2k+1$, делим на 2 вершины, если $0$, то 1 ребёнок, он новый корень, если два ребёнка, один $k$, другой $k-1$, то соединяем с корнем, получаем корень размера $2k$.
    * Плюсы: во-первых, ожидаемая глубина ниже, так как узел заполнен на 2/3, а не на 0.5 от максимума. Во-вторых, при переполнении в B сразу делим узел, а тут пытаемся отщипнуть и перекинуть, меньше созданий новых узлов.
* `B+-дерево`: в узлах храним только ключ (а не пару ключ-значение), а значения храним только в листьях...
* `2-3-Tree`: B-дерево, у каждой вершины 2 или 3 сына.
* `2-3-4 Tree`: разрешаем вершине иметь 4 сына.
* `RB-дерево` описано выше, рассмотрим `доказательство` его эквивалентности `2-3-4-Tree`. Соединим красные вершины с их черными отцами, получим толстую вершину с 1-3 ключами и 2-4 детьми. В обратную сторону: у вершины 2-3-4-дерева с двумя ключами создадим правого красного сына, а с тремя ключами - двух красных сыновей. Но это не биекция, так как если у корня один красный сын, то теряется инфа о том правый он или левый.
* `AA-дерево`: преобразуем 2-3-дерево по тому же принципу, что и 2-3-4, в RB-дерево. Получим такое дерево, у которого люая красная вершина - правый сын своего отца. Из Андерсона я понял, что перфоманс одинаковый, но RB более стабильная (наверн константа хорошая), а у АА поменьше глубина и поиск работает побыстрее.
### <span style="color:green">(a)</span> 25. Treap. Два определения случайного дерева (RBST). Эквивалентность определений. Оценка средней глубины вершины.
* `RBST` - каждый элемент может с равной вероятностью стать корнем. Аналогично рекурсивно сверху вниз по всем поддеревьям.
* `RBST` - результат добавления случайной перестановки в пустое BST.
* Если $x_i$ различны, определения `эквивалентны`.
    * `Д-во`: Корень - первый элемент перестановки, выбирается равновероятно, левое поддерево - случайная перестановка, и дальше по индукции.
* Хотим, чтобы матожидание максимальной глубины вершины (высоты) было $\mathcal{O}(\log n)$. Доказали в теорке...
* Но можем доказать, что для каждого $v_i$ матожидание глубины узла равно $\mathcal{O}(\log n)$ = оценка средней глубины вершины.
    * `Док-во`: Рассмотрим путь от корня до $v_i$. При каждом спуске с вероятностью хотя бы 1/3 размер поддерева уменьшается в полтора раза, т.к. из отсортированного множества ключей корень с вероятностью 1/3 выбран из второй трети, и в поддереве будет не больше 2/3 всех ключей. Тогда количество таких удачных спусков, нужных чтоб прийти в лист, $\log_{1.5}(n)$. Учитывая, что "удачный" ход происходит с вероятностью 1/3, матожидание количества спусков до корня - $3\log{1.5}(n)=\mathcal{O}(\log n)$.
### <span style="color:green">(a)</span> 26. Treap. $\exists!$ дерево при уникальных $y$. Связь со случайными деревьями. Операции Split, Merge.
* `Treap`, он же `Декартач` от пар $\langle x_i, y_i \rangle$ - структура, являющаяся BST по $x$ и кучей с минимумом в корне по $y$.
* Если все $y_i$ различны, декартово дерево единственно
    * `Д-во`: Корень однозначно - минимальный $y$. Левое поддерево - декартач всех иксов меньших корня, единственно по индукции, как и правое поддерево.
* `Treap` от множества ключей - декартач от пар $\langle x_i, random \rangle$, и как BST является RBST (что даёт матожидание высоты, глубины любой вершины и средней глубины вершины $\mathcal{O}(\log n)$).
* `Split`: 
```cpp
void Split(Node* t, Node* &l, Node* &r, int x) {
    if (!t) l = r = 0;
    else if (t->x < x) Split(t->r, t->r, r, x), l = t;
    else Split(t->l, l, t->l, x), r = t;
}
```
* `Merge`: делает корнем либо корень L, либо R, Пусть L, тогда рекурсивно делаем `Merge(L.R, R)`.
### <span style="color:blue">(b)</span> 27. Treap. Эффективная реализация Add, Delete. Персистентные декартовы деревья.
* `Add`: Спускаемся по дереву до позиции вставки $v$, ставим вершину с $x$ вместо $v$, делаем её детьми пару `Split(v, x)`.
* `Delete`: Спускаемся до удаляемой, и заменяем её на `Merge` детей.
* `Персистентность`: Любую СД можем представить как множество массивов и BST, BST само по себе может быть персистентным, а сделать таковым массив можно двумя стульями:
    * BST по неявному ключу (Rope)
    * ДО с запросами сверху.
* Если хотим только присваивать и читать, лучше второй способ.
### <span style="color:blue">(b)</span> 28. Garbage collector-ы на примере персистентного treap (ссылочный, стековый).
* `Garbage collection` - неиспользуемые ноды удаляем, когда кончается память. Есть 2 стула:
    * `Списковый аллокатор`: Поддерживаем список свободных Нод, добавим параметр used. Когда станет мало памяти, пометим нужные корни и от них dfs-ом всё используемое как нужное, остальное удалим.
    * `Стековый аллокатор`: Выделим стек на столько нод, сколько есть в памяти. Когда останется мало памяти, dfs-ом выпишем в отдельный массив содержимое текущего дерева, освободим эту память, и заново построим дерево по массиву.
### <span style="color:green">(a)</span> 29. Splay. Три вида поворотов. Операции Splay, Add. Формулировка теоремы о времени работы.
* `Splay-дерево`: самобалансирующееся BST, не хранящее в вершине никакой доп.информации. В худшем случае глубина линейна, но амортизированное время всех операций - логарифм.
* `Три вида поворотов`: 
    * `Zig`: Single-rotation, когда у узла нет деда.
    * `Zig-Zig`: узел и его родитель оба левые или правые дети, 2 последовательных однонаправленных поворота.
    * `Zig-Zag`: Узел и его родитель - дети разных сторон, два разнонаправленных поворота.
* `Splay(v)`:
    * Пока $v$ - не корень:
        * Если нет дедушки: `Zig(v)`.
        * Если v и родитель на одной стороне: `Zig-Zig(v)`.
        * Иначе: `Zig-Zag(v)`.
* `Add`:
    * Обычная вставка $x$ в качестве листа
    * `Splay(x)`
* `Теорема о времени работы`: амортизированное среднее время работы 1 операции $\mathcal{O}(\log n)$.
### <span style="color:blue">(b)</span> 30. Splay. Del, Split, Merge. Потенциал. Оценка всего, кроме операции Splay.
* `Del`:
    * `Splay(x)` и убрать, остались 2 дерева $L$ и $R$.
    * Если $L$ - пусто, дерево - $R$.
    * Иначе нашли $m:=\max(L)$, `Splay(m)`, подсоединили $R$ как правое поддерево.
* `Split`:
    * `Splay(x)`
    * Корень и слева $\leqslant x$, справа $>x$.
* `Merge`:
    * `Splay(max(L))`, подсоединили `R` как правое.
* `Потенциал`: 
    * Ранг узла $R_v = \log(\text{size}_v)$, потенциал $\phi = \sum_v R_v$.
    * Для пустого дерева равен нулю, а так всегда больше либо равен нулю.
* `Оценка операций`:
    * `Split/Merge/Add/Del` делают $\mathcal{O}(1)$ операций Splay, и ещё $\mathcal{O}(1)$ операций.
    * Тогда их амортизированное время $\log n$.
### <span style="color:red">(c)</span> 31. Доказательство амортизированной оценки операции Splay.
* `Леммы`:
    * $\forall x,y>0, x+y=1: \log x+\log y \leqslant -2$.
        * `Д-во`: $\log x + \log y = \log x(1-x) \leqslant \log 1/4 = -2$
    * Обобщение: $x + y = C \Rightarrow \log x + \log y \leqslant 2\log C - 2$.
        * `Д-во`: $\log x + \log y = 2 \log C + \log x/C + \log y/C \leqslant 2 \log C - 2$.
* `Потенциал`:
    * Оцениваем изменение потенциала после каждого вращения
    * Для `Zig-Zig` (остальное аналогично): $a = t + \Delta \phi \leqslant 2 + (R_{x'} + R_{y'} + R_{z'}) - (R_{x} + R_{y} + R_{z}) \leqslant 2 + R_{x'} + R_{z'} - 2R_x = F$.
    * $R_{z'} = \log(C+D+1), R_x = \log (A+B+1) \Rightarrow R_{z'} + R_x \leqslant 2R_{x'} - 2$ (лемма).
    * $2+R_{x'}+R{z'}-2R_x=F\leqslant 3(R_{x'}-R_x)$.
* Тогда `Th.`: $a_{v\to u} \leqslant 3(R_u-R_v)+1=3\log \frac{\text{size}_u}{\text{size}_v}+1$.
* `Следствие`: За $m$ операций $\sum a_i=\mathcal{O}(m\log n) \Rightarrow$ амортизированный логарифм на операцию.
### <span style="color:red">(c)</span> 32. Оценка $\sum k_i\log \frac{m}{k_i}$ с доказательством.
* Запрос к узлу $v$ происходит $k_v$ раз, всего $m=\sum k_v$.
* Из теоремы про `Splay`, с $\text{size}_v=k_v$ получаем суммарное время $\mathcal{O}(m + \sum k_i\log \frac{m}{k_i})$.
### <span style="color:red">(c)</span> 33. Теорема о статической оптимальности. Гипотеза динамической оптимальности.
* `Статическая оптимальность`: даны пары $(x_i, p_i)$, где $p_i$ - вероятность обращения к ключу $x_i$, $d_i$ - глубина ключа $x_i$. Хотим минимизировать $\sum_i p_i d_i$. 
    * Можно решить динамикой по подотрезкам за куб: $f[l,r] = \min_{root}((\sum_{i\in [l,r]} p_i) - p_{root} + f[l, root-1] + f[root+1, r])$.
    * Кнут доказал, что $f[l, r-1] \leqslant f[l, r] \leqslant f[i + 1, r]$, что позволяет оптимизировать до квадрата.
* `Splay` отличается от статически оптимального BST всего лишь в константу раз.
* Гипотеза динамической оптимальности: если между запросами можно вращать дерево, то `Splay` дерево в худшем случае работает за $\mathcal{O}(n + \text{OPT})$.
## Структуры данных

### <span style="color:green">(a)</span> 34. Persistent. Offline, дерево версий.
* Если все запросы известны заранее, можем построить дерево версий и обойти его поиском в глубину. При выходе из рекурсии нужно откатывать операции, а значит амортизации убиваются (напр. СНМ будет работать за чистый логарифм).
### <span style="color:green">(a)</span> 35. Persistent. Детская персистентность (2 версии), частичная персистентность за $\mathcal{O}(\log n)$.
* `Детская персистентность`:
    * Обращение за единицу, изменение за линию: просто копиуем массив полностью.
    * Изменение за единицу, обращение за линию: храним дерево версий, для каждой версии - версию отца и изменение относительно отца
* `Частичная персистентность`: в каждой ячейке хранить последовательность изменений, добавление - `push_back`, запрос - бинпоиск.
### <span style="color:green">(a)</span> 36. Persistent. Online Fully Persistent массив за $\mathcal{O}(\log n)$. Любой способ.
* Персистентное AVL по неявному ключу есть персистентный онлайн массив с кучей ништяков за $\mathcal{O}(\log n)$.
### <span style="color:blue">(b)</span> 37. Persistent. Дек через Pairing.
* Храним в структуре не более одного элемента слева, не более одного справа, а остальные бьём на пары и рекурсивно делаем то же самое.
```cpp
struct Deque<T> {
    T l;
    Deque<pair<T, T>> m;
    T r;

    Deque<T> push_back(T x) {
        if (r == null) return {l, m, x};
        return Deque(l, m.push_back(pair(r, x)), null);
    }
    pair <T, Deque<T>> pop_back() {
        if (r != null) return pair(r, Deque(l, m, null));
        if (m == null) return pair(l, Deque(null, null, null));
        <x, m1> = m.pop_back();
        return pair(x.second, Deque(l, m1, x.first));
    }
} 
```
### <span style="color:blue">(b)</span> 38. Persistent. СНМ. Персистентный массив: BST vs дерево отрезков.
* `Персистентное СНМ` будет работать за чистый логарифм из-за отката операций при выходе из рекурсии по дереву версий.
* `Персистентный массив`:
    * `BST`: дерево поиска по неявному ключу.
    * `Дерево отрезков`: проще в реализации, если нужно только присваивание и чтение.
### <span style="color:red">(c)</span> 39. Persistent. Стек, очередь, дек за $\mathcal{O}(1)$ (через 5 стеков).
* `Персистентный стек` с операциями push/pop/size/copy за $\mathcal{O}(1)$ - дерево.
* Умеем убивать амортизацию в очереди на 2 стеках.
* Напишем персистентную очередь на 5 стеках:
```cpp
struct Queue {
    Stack L, R; // L - pop, R - push
    Stack L1, R1, tmp; // for copying
    int state, copied;

    Queue Copy() const {
        return Queue(L.copy(), R.copy(), L1.copy(), R1.copy(), tmp.Copy(), state, copied);
    }

    int Front() const {
        return L.front();
    }

    pair <Queue, int> Pop() const {
        Queue res = Copy();
        int data = res.L.pop();
        for(int i = 0; i < 3; ++i) res.Step();
        return make_pair(res, data);
    }

    Queue Push(int data) const {
        Queue res = Copy();
        res.R.push(data);
        for(int i = 0; i < 3; ++i) res.Step();
        return res;
    }

    void Step() {
        if (state == DO_NOTHING) {
            if (L.size > R.size) return;
            R1 = R, R = new Stack(), tmp = new Stack();
            state = REVERSE_R;
        }
        if (state == REVERSE_R) {
            tmp.push(R1.pop());
            if (R1.size == 0) L1 = L.copy(), state = REVERSE_L;
        } else if (state == REVERSE_L) {
            R1.push(L1.pop());
            if (L1.size == 0) copied = 0, state = REVERSE_L_AGAIN;
        } else {
            if (L.size > copied) copied++, tmp.push_back(R1.pop());
            if (L.size == copied) L = tmp, state = DO_NOTHING;
        }
    }
}
```
### <span style="color:blue">(b)</span> 40. Rope. Skip-List. Операции Find, Insert.
* `Rope` - интерфейс СД, должен уметь `insert`, `erase`, `split`, `merge`, `rotate`. Это умеют сбалансированные деревья по неявному ключу со split и merge, `skip-list` и всякие sqrt-декомпозиции.
* `Skip-list` - $\log n$ односвязных списков, в нижнем содержатся все, каждый элемент из $i$-го списка с вероятностью 0.5 содержится в $(i+1)$-м, общий размер $2n$.
```cpp
struct Node {
    Node *down, *right;
    int x, right_len;
};
const int LOG_N = 17;
vector<Node*> head[LOG_N+1];
Node* find(int i) {
    Node* res;
    int pos = -1;
    for (Node*v = head[LOG_N]; v; res = v, v = v->down)
        while(v->right && pos + v->right_len < i)
            pos += v->right_len, v = v->right;
    return res;
}
```
* Матожидание `Find` - $\mathcal{O}(\log n)$.
    * `Д-во`: на каждом шаге форика матожидание числа шагов $\mathcal{O}(1)$.
* `Insert`: вызовем `Find` для каждого уровня и запомним массив `prev`; с вероятностью 0.5 поднимаемся выше, на каждом уровне создаём новый узел, вставляем после `prev`, настраиваем `right` и `down`, обновляем `right_len`. Сложность логарифм.
### <span style="color:red">(c)</span> 41. Rope. Skip-List. Операции Erase, Split, Merge.
* `Erase`: вызываем `Find` на каждом уровне и сохраняем `prev`, если элемент существует на текущем уровне, удаляем его из псписка и соединяем соседей ссылками, если отсутствует - просто уменьшаем `right_len`. Сложность логарифм.
* `Split`: идём как `find`, и отсоединяем правую часть от найденного узла на каждом уровне. Cложность логарифм.
* `Merge`: приклеиваем соответствующие уровни одного списка к уровням другого, сложность так же логарифм.
### <span style="color:green">(a)</span> 42. SQRT. Корневая декомпозиция. Статичная по массиву (с примером).
* `SQRT-decomposition`: просто разобьём массив на $\sqrt{n}$ кусков размера $\sqrt{n}$.
* `RSQ` умеем теперь решать за $\mathcal{O}(1)$ на изменение (поменяли сумму нужного куска) и $\mathcal{O}(\sqrt{n})$ на запрос (левый хвост, правый хвост по корню и в середине максимум корень кусочков).
* `RMQ` за $\mathcal{O}(\sqrt{n})$ за оба запроса, так как не можем быстро пересчитать минимум.
### <span style="color:blue">(b)</span> 43. SQRT. Корневая декомпозиция. Через Split/Merge. Через Split/Rebuild. Примеры.
* `Split/Merge`: Хотим добавлять и вынимать элементы. Тогда храним вектор кусков массива. При операции вставки/удаления сначала ищем какой кусок надо менять, затем с ним делаем что нам нужно за $\mathcal{O}(\sqrt{n})$.
    * Он мог стать слишком маленьким или большм. Если меньше корня, мерджим с соседом, если больше 2 корней, сплитим на два (и то и то очев умеем за корень с пересчетом целевой функции).
    * По итогу удерживаем размер куска от корня до двух коней и количество всегда $\Theta(\sqrt {n})$
    * `Время` старых операций осталось прежним, а новые работают за $\mathcal{O}(\sqrt(n))$.
* `Split/Rebuild`: Операция `split(i)` - сделать $i$ началом куска. Тогда можем в запросах писать `split(l)` и `split(r)` и не париться с хвостами.
    * Число кусков могло стать слишком большим. Если их $\geqslant 3 \sqrt{n}$, вызываем `rebuild` - построим заново декомпозицию за $\mathcal{O}(n)$, но вызываем её не чаще чем раз в корень действий, поэтому среднее время запроса остаётся корень.
### <span style="color:red">(c)</span> 44. SQRT. Оптимальный выбор размера куска на примере задачи (kth_stat + reverse).
* Не всегда выгодно разбивать ровно на корень частей по корню элементов. Обозначим число кусков за $k$, тогда в каждом куске $m:=n/k$ элементов. Соптимизируем $k$ на примере задачи с $k$-ой статистикой на отрезке и реверсом. Времена операций тогда:
    * `inner_split`: $\mathcal{O}(m \log m)$.
    * `split(i)`: $\mathcal{O}(k)$ + `inner_split` = $\mathcal{O}(k + m \log m)$.
    * `reverse/add_to_all`: $\mathcal{O}(k)$ + `split(i)` = $\mathcal{O}(k + m \log m)$.
    * `insert/erase`: $\mathcal{O}(k + m)$.
    * `sum`: $\mathcal{O}(m + k \log m)$.
* Суммарное время запросов - $\mathcal{O}((k+m)\log m)$. В худшем случае будут все запросы по очереди, асимптотика достигается. А ещё каждые $k$ запросов вызывается `rebuild` и работает за $\mathcal{O}(n \log n)$.
* Хотим минимизировать: $\mathcal{O}((k + m)\log m + \frac{1}{k} n \log n)$.
* Логарифмы считаем асимптотически равными, и минимизируем $(\frac{n}{k}+k)\log n$.
* хотим минимизировать $n/k + k$, одна величина убывает, другая возрастает, значит сумма оптимальна при $n/k=k$, то есть $k=\sqrt{n}$.
* Если без `rebuild`, то время $\Theta(m + k \log m)$, заменим логарифм $m$ на $n$, сумму на максимум и решаем $\frac{n}{k}=k\log n$, и $k=\sqrt{n / \log n}$.
### <span style="color:red">(c)</span> 45. Корневая декомпозиция по запросам на примере «Dynamic Connectivity Offline за $\mathcal{O}(m \sqrt{m})$».
* `Dynamic Connectivity Offline`: дан неорграф, 3 типа запросов: добавить ребро в граф, удалить ребро, проверить связность двух вершин. Нужно обработать $m$ запросов, обрабатываем пачками по корню.
* `Идея решения`: разбить время на блоки размера $k$, какие-то рёбра есть всегда в промежутке (статические), каких-то никогда нет (отсутствующие), и лишь не более чем $k$ появляются и пропадают (динамические).
* `DFS`:
    * Разбиваем время на блоки размера $\sqrt{m}$.
    * В блоке конденсируем для статических рёбер за $\mathcal{O}(m)$.
    * Обрабатываем запросы внутри блока: берём активные рёбра из динамических, запускаем `DFS` на них + сжатых компонентах, сложность $O(k)$ на запрос.
    * В сумме $\mathcal{O}(km+m^2/k)=\mathcal{O}(m \sqrt{m})$ при оптимальном $k=\sqrt{m}$.
* `DSU`:
    * Разбиваем время на блоки.
    * Добавляем в `DSU` статические рёбра.
    * Обрабатываем запросы внутри блока: добавляем активные рёбра, проверяем связность, откатываем `DSU`.
    * Итого сложность $\mathcal{O}(m\sqrt{m} \alpha(m))$.
### <span style="color:green">(a)</span> 46. Дерево отрезков. Реализация сверху. Оценка на память, на время get.
* `Построение`: Корень - отрезок $[0, n-1]$, левый сын - левая половина отрезка, правый сын - правая, в листьях элементы массива.
* `Память`: $2^{1+\lceil \log_2 n \rceil}\leqslant 4n$.
* `Get`: Посещает $\mathcal{O}(\log n)$ вершин, разбивая отрезок на $\leqslant 2\log n$ непересекающихся отрезков.
### <span style="color:green">(a)</span> 47. Дерево отрезков. Массовые модификации. += и getMin. = и getSum.
* `Отложенные операции`: храним помимо `value` ещё и `add` (неприменённое действие), `push`: протолкнуть действие в детей.
```cpp
void push(int v) {
    if (add[v] == 0) return;
    t[2*v] += add[v]; add[2*v] += add[v];
    t[2*v+1] += add[v]; add[2*v+1] += add[v];
    add[v] = 0;
}

void rangeAdd(int v, int vl, int vr, int l, int r, int x) {
    if (vr < l || r < vl) return;
    if (l <= vl && vr <= r) { add[v] += x; t[v] += x; return; }
    push(v);
    int vm = (vl + vr) / 2;
    rangeAdd(2*v, vl, vm, l, r, x);
    rangeAdd(2*v+1, vm+1, vr, l, r, x);
    t[v] = min(t[2*v], t[2*v+1]);
}
```
### <span style="color:blue">(b)</span> 48. Дерево отрезков. Реализация снизу. Корректность. Сравнение с реализацией сверху.
* `Построение`: $t_{[n\dots 2n-1]}$ - изначальный массив, для родителя справедливо $t_i=\min(t_{2i}, t_{2i+1})$.
* `Память`: $2n$ ячеек...
* `Корректность`: по сути наше ДО - это лес, состоящий из слоёв. Нужно доказать, что слои не пересекаются.
    * `Д-во`: нижний слой - исходный массив $[l_0, r_0) = [n, 2n)$, $r_0\leqslant 2l_0$, по индукции докажем что $r_i\leqslant 2l_i$. Слой отцов - $[\lceil l/2 \rceil, \lceil r/2 \rceil)$, они не пересекаются значит $r_{i+1}\leqslant l_i$, $r_i\leqslant 2l_i$.
* Реализация снизу хавает меньше памяти, `Get` работает за логарифм от отрезка, а не от $n$, а ещё без рекурсии.
* К реализации сверху проще прикрутить отложенные операции, а ещё оно реально дерево, а ещё его можно сделать динамическим или персистентным.
### <span style="color:blue">(b)</span> 49. Дерево отрезков. Реализация сверху: ровно $2n$ ячеек.
* Если $n=2^k$, всё хорошо, памяти ровно $2n$, иначе возникают проблемы.
* Решается тем, что если мы $v$, левый сын $v+1$, а правый $v+2(vm-vl)$.
### <span style="color:blue">(b)</span> 50. Дерево отрезков. Сравнение с BST по неявному ключу.
* Также пишется на указателях.
* Также все запросы сверху и Ноды нельзя менять.
* BST умеет ещё `insert/delete/split/merge/reverse`.
* А ещё BST лучше по памяти.
### <span style="color:red">(c)</span> 51. Дерево отрезков. Динамическое (два способа), сжатие координат. Применение сжатия координат для задач со Scanline.
* Пусть есть массив длины $10^18$, и изначально заполнен нулями, хотим ДО на нём.
    * Первый способ: все массивы заменить на хеш-таблицы, не будет лишнего кода, но константа времени работы увеличится.
    * Второй способ: вместо хранения в массиве храним ноды с указателями на детей, и создаём вершины лениво, если запрос попадает в `NULL`, создаём на этом месте Ноду.
* Сжатие координат: для $n$ запросов в офлайне у нас $\mathcal{O}(n)$ интересных точек, сложим их в массив и будем всё индексировать по ним. Свели исходную задачу к задаче на массиве длины $\mathcal{O}(n)$.
### <span style="color:green">(a)</span> 52. Scanline. 2D запрос на массиве через персистентную структуру для каждого префикса.
* `Персистентное ДО`: проходим сканлайном по $i$, создавая новую версию ДО с $y_i=a_i$, ответ на запрос - разность версий `tree[R]-tree[L-1]`.
### <span style="color:green">(a)</span> 53. Scanline. Число точек в прямоугольниках; число прямоугольников, накрывающих точку.
* `Число точек в прямоугольниах`:
    * Сначала разобьём каждый прямоугольник на 2 горизонтальных стакана: с теми же игреками от $x_1$, и от $x_2$ тогда целевая функция `f(rect)=f(stack2)-f(stack2)`.
    * Все точки и стаканы сортируем по иксам.
    * Проходим сканлайном слева направо, обрабатывая 2 типа событий:
        * Точка: $count[y] += 1$.
        * Конец стакана: посчитали $\sum_{y\in[y_1;y_2]}$ count[y].
* `Число прямоугольников, покрывающих точку`:
    * Для начала/конца прямоугольника, на дереве отрезков сделаем +-1 на отрезке $[y_1, y_2]$, в точке смотрим на `count[y]` - это ответ.
### <span style="color:blue">(b)</span> 54. Scanline. Площадь объединения прямоугольников.
* `Scanline` по $x$, события по базе:
    * Начало rect: cnt[] += 1 на отрезке $y$.
    * Конец rect: cnt[] -= 1 на отрезке $y$.
* При переходе $x_i\to x_{i+1}$, умножаем разность на вертикальную длину покрытой прямоугольниками части (== вся прямая - число нулей).
### <span style="color:blue">(b)</span> 55. Scanline. Наибольшая по сумме элементов возрастающая подпоследовательность.
* `DP`: $dp[i] = a[i] + \max_{j<i,a[j]<a[i]}dp[j]$.
* Прикрутим `ДО` по $x$ `ДО`-шек по $dp[x]$ и каждый раз запросом на отрезке можем найти максимум.
### <span style="color:red">(c)</span> 56. Scanline. Лежит ли точка внутри невыпуклого многоугольника?.
* Пусть `ДО` для каждого `y` хранит количество пересекающих его рёбер. Тогда для точки смотрим на сумму выше, если нечётное, точка внутри, иначе снаружи.
### <span style="color:green">(a)</span> 57. 2D. ДО сортированных массивов. Связь с 2D запросом (кол-вом точек в прямоугольнике).
* `Задача`: Количество точек в прямоугольнике.
* `Решение`:
    * `ДО` отсортированных массивов (в каждой вершине массив для отрезка).
    * `Запрос`: Разбиваем на непересекающиеся подотрезки (по базе), и в каждом делаем бинпоиск, ответ сумма. Сложность: логарифм квадрат на запрос.
* `Связь с 2D`: Точки $(x_i, y_i)$ то же самое что и массив $a[i]=y_i$, и это как запрос на массиве количества $y_i$ принадлежащих отрезку в $i\in [L,R]$.
### <span style="color:blue">(b)</span> 58. 2D. Количество точек в прямоугольнике online за $\langle \mathcal{O}(n \log n), \mathcal{O}(\log n)\rangle$.
* Персистентное `ДО`, запрос: разность двух версий. 
### <span style="color:blue">(b)</span> 59. 2D. Поиск $k$-й порядковой статистики на отрезке за $\mathcal{O}(\log^2 n)$.
* Бинпоиск по ответу $x$.
* Внутри 2D-запрос количества элементов $\leqslant x$ на $[L,R]$, который теперь умеем за логарифм, итого квадратный логарифм.
### <span style="color:red">(c)</span> 60. 2D. Многомерные структуры. ДО из ДО (зачем нужно?), ДО из ДД (зачем нужно?). Трёхмерное ДО, k-мерное ДО.
* `ДО из ДО`: для 2D-запросов, занимает $n \log n$ памяти и отвечает на запрос за квадратный логарифм.
* `ДО из ДД`: на случай если есть запросы на изменение, память то же, запрос логарифм квадрат.
* `k-мерное ДО`: для ответа на kD-запросы, память $n \log^{k-1} n$, время $\log^k n$.
### <span style="color:red">(c)</span> 61. 2D. Поиск k-й порядковой на отрезке за $\mathcal{O}(\log n)$.
* Вместо бинпоиска, используемого нами в квадратичном решении, давайте параллельно спускаться по деревьям версий $r+1$ и $l$, пусть сейчас мы в $a$ и $b$. Тогда обеим вершинам соответствует один отрезок, если `a->l->sum - b->l->sum < k`, спускаемся вправо и ищем там `k-(a->l->sum - b->l->sum)`-е число, иначе в обоих деревьях спускаемся влево.

## RMQ, LCA, LA
### <span style="color:green">(a)</span> 62. RMQ. Разреженные таблицы (Sparse Table).
* `Разреженная таблица` - структура данных для `RMQ` на статическом массиве за единицу с предподсчётом $\mathcal{O}(n \log n)$.
* $f[k, i]$ - минимум на отрезке $[i, i+ 2^k)$. Массив $f$ предподсчитывается за $\mathcal{O}(n \log n)$:
    * $f[0]$ - исходный массив.
    * $f[k, i] = \min(f[k-1, i], f[k-1][i+2^{k-1}])$.
* Для ответа на запрос находим $\max_k(2^k \leqslant (r - l + 1))$ (можем предподсчёт $\log (a)$) и ответом будет $\min(f[k][l], f[k][r - 2^k + 1])$.
* Получили решение `static rmq` за $\langle n \log n, 1 \rangle$.
### <span style="color:blue">(b)</span> 63. RMQ. Фенвик. Операции. Корректность.
* `Дерево Фенвика`: СД, поддерживает операции `update` и `prefix_query`.
* Каждая вершина хранит целевую функцию на отрезке `[i & (i + 1), i]`, при `update` идем вперед по `i |= i + 1`, а при запросе назад, `i = (i & (i + 1)) - 1`.
### <span style="color:green">(a)</span> 64. RMQ. Сведение LCA $\to$ RMQ $\pm1$. Решение задачи LCA за $\langle \mathcal{O}(n \log n), \mathcal{O}(1) \rangle$.
* Эйлеров обход дерева: выписываем вершины в порядке `DFS`, сохраняя глубины. Массив глубин обладает свойством $\pm1$, тогда `LCA(u, v)` - то же, что `RMQ` между первыми вхождениями `u` и `v`.
* Используем разреженную таблицу и с предподсчётом $\mathcal{O}(n \log n)$ сможем отвечать на запросы за $\mathcal{O}(1)$.
### <span style="color:blue">(b)</span>  65. RMQ. Модификации разреженных таблиц. $\langle \mathcal{O}(n \log \log n), \mathcal{O}(1) \rangle$ и $\langle \mathcal{O}(n), \mathcal{O}(\log \log n) \rangle$.
* Разобьём массив на куски длины логарифм, минимум на $i$-м куске - $b_i$.
* Тогда любой отрезок разбивается на отрезок $b$ и два "хвоста".
* Минимум на хвосте - минимум на префиксе или суффиксе одного куска, все такие частичные минимумы предподсчитаем за $O(n)$. Теперь на $b$ построим `Sparse Table` размера $\leqslant n$, имеем $\langle n, 1 \rangle$ решение.
* Для отрезков, полностью попадающих в один из кусков, можем построить структуру данных для поиска минимума и получить следующие оценки на общее время работы:
    * `Просто цикл`: $\langle n, \log n\rangle$
    * `Sparse Table`: $\langle n \log \log n, 1\rangle$
    * `SegTree`: $\langle n, \log \log n \rangle$
### <span style="color:blue">(b)</span> 66. RMQ. Решение задачи RMQ за $\langle \mathcal{O}(n \log^* n), \mathcal{O}(\log^* n) \rangle$.
* Берём структуру из предыдущего билета и рекурсивно строим её от кусков размера $\log n$, и так далее. Получаем сложность $\langle n\log^*n, \log^*n \rangle$.
### <span style="color:blue">(b)</span> 67. RMQ. Сведение RMQ $\to$ LCA. Построение декартового дерева за линейное время.
* Построим декартач на парах $(i, a_i)$, пары отсорчены по $x$, поэтому декартач строится за линию.
* `RMQ` на $[l, r]$ в исходном массиве равно `LCA`(l, r) в полученном дереве.
    * `Д-во`: Каждой вершине декартова дерева соответствует отрезок исходного массива, корень поддерева - минимум по $y$ на этом отрезке. Будем спускаться от корня, пока не встретим вершину, которая разделяет $l$ и $r$, эту вершину как $i$, её отрезок как $[L_i, R_i]$: $a_i = \min_{L_i\leqslant j \leqslant R_i}a_j$, $L_i\leqslant l \leqslant i \leqslant r \leqslant R_i, $a_i\geqslant \min_{l\leqslant j \leqslant r}a_j$, $i \in [l, r]$, $i = \text{LCA}(l, r)$.
### <span style="color:red">(c)</span> 68. RMQ. Решение RMQ за $\langle \mathcal{O}(n), \mathcal{O}(1) \rangle$ для $n \leqslant w$, хранение стеков минимумов.
* Разобьём на куски длины 32, построим сверху `Sparse Table`. Для этих кусков длины 32 сделаем предподсчёт: пройдём слева направо, поддерживая стек элементов, которые являются минимумами на суффиксах и маску, какие элементы принадлежат этому стеку, для каждого префикса заполним маску.
* Пример на с.211 п. 29.2 конспекта.
* Предподсчёт стеков и масок работает за линеию, суммарное время для предподсчёт $n + n/32 \log n + n = \mathcal{O}(n)$.
* Теперь любому подотрезку $[L,R]$ куска длины 32 соответствует маска префикса $[0, R]$ у которой отбрасываем первые $L$ бит и ищем крайнюю левую единицу - это или предподсчёт, или 1 процессорная инструкция.
### <span style="color:red">(c)</span> 69. RMQ. Фарах-Колтон-Бендер. Метод четырёх русских, решение RMQ и LCA за $\langle \mathcal{O}(n), \mathcal{O}(1) \rangle$.
* `RMQ+-1` умеем решать, разбив массив на куски длины $(\log n) / 2$, минимум на i-м куске обозначив за $b_i$, после этого $\min(a_1, a_2, \dots a_k) = a_1+\min(0, a_2-a_1, a_3-a_1\dots a_k-a_1)=a_1+X$. Изза $\pm1$  свойства, $X$ - минимум на одной из $2^{k-1}$ последовательностей, их можно предподсчитать за $2^k=\sqrt{n}=o(n)$. Ну а потом построим на кусочках `Sparse Table`. Это Фарах-Колтон-Бендер.
* Серией сведений `RMQ`$\to$`LCA`$\to$`RMQ+-1` научились решать `RMQ` за $\langle n, 1 \rangle$.
### <span style="color:red">(c)</span> 70. RMQ. Фарах-Колтон-Бендер: предподсчёт ровно за $\mathcal{O}(2^k)$.
* Нет слов, один код
```cpp
for (int mask = 0; mask < (1 << k); ++mask)
    auto [i, m] = ans[mask / 2];
    ans[mask] = {i + 1, m + (mask % 2 ? -1 : +1)};
    if (ans[mask].second > 0)
        ans[mask] = {0, 0};
for (int i = n; i >= 1; i--)
    add_bit(mask[i / k], a[i] - a[i-1] == 1 ? 0 : 1) // младший
int get(l, r)
    int il = l/k;
    int ir = r/k;
    if (il != ir)
        return suf + sparse + pref
    auto [i, m] = ans[(mask[il] >> (l % k)) & ((1 << (r % k - l % k)) - 1)];
    return i + l % k;
```
### <span style="color:green">(a)</span> 71. LCA. Двоичные подъёмы. Решение LCA, два способа (с 1-й двигающейся вершиной и с 2-мя).
* `Двоичные подъёмы`: $up[k][v] = up[k - 1][up[k - 1][v]] - прыжок на $2^k$ это 2 прыжка на $2^{k-1}$. База: `up[v][0] = parent[v]`. Переход выше, для крайних случаев `up[0][root]=root`.
* Теперь LCA снова уравнять глубины и вместе прыгать вверх:
```cpp
int lca(int a, int b) {
    if (depth[a] < depth[b]) swap(a, b);
    a = jump(a, depth[a] - depth[b]);
    for (int k = K - 1; k >= 0; k--) {
        if (up[k][a] != up[k][b])
            a = up[k][a], b = up[k][b];
    }
    return a == b ? a : up[0][a];
}
```
* Можно с одной двигающейся вершиной:
```cpp
int lca(int a, int b) {
    for (int k = K - 1; k >= 0; k--) {
        if (!isAncestor(up[k][a], b))
            a = up[k][a]
    }
    return isAncestor(a, b) ? a : up[0][a];
}
```
### <span style="color:blue">(b)</span> 72. LCA в offline, алгоритм Тарьяна.
* Для каждой вершины построим список запросов с ней связанных: `q[a[i]].push_back(i), q[b[i]].push_back(i)`.
* Будем обходить дерево dfs-ом, перебирать запросы для вершины и отвечать на те, второй конец которых серый или чёрный:
```cpp
void dfs(int v) {
    color[v] = GREY;
    for (int i : q[v]) {
        int u = a[i] + b[i] - v; // other one
        if (color[u] != WHITE) 
            answer[i] = DSU.get(u);
        for (int x : graph[v])
            dfs(x), DSU.parent[x] = v;
        color[v] = BLACK;
    }
}
```
* Серыми вершинами выстлан путь от корня до `v`, у каждой такой вершины есть чёрная часть поддерева и это её множество в `DSU`. `LCA(u, v)` - всегда сервая вершина, то есть нужно подниматься от `u` до ближайшей серой вверх, что мы и делаем.
* Асимптотика $\mathcal{O}((m+n)\alpha)$
### <span style="color:green">(a)</span> 73. LA. Решение LA в online и в offline.
* `LA(v, k)` - ~~улететь в Los Angeles~~ подняться от вершины $v$ на $k$ шагов вверх.
* В `online` уже умеем за $\langle n \log n, \log n \rangle$ двоичными подъёмами.
* В `offline` на $m$ запросов можно ответить dfs-ом за $\mathcal{O}(n + m)$: когда dfs в $v$, у нас в стеке весь путь до корня и можем обратиться к люому элементу за $\mathcal{O}(1)$.
### <span style="color:blue">(b)</span> 74. LA. Алгоритм Вишкина (решение LA за $\langle \mathcal{O}(n), \mathcal{O}(\log n) \rangle$).
* Как и при сведении `LCA` к `RMQ+-1`, выпишем Эйлеров обход с глубинами. Тогда `LA(v, k) = getNext(index[v], height[v] - k)`, где `index` - позиция в эйлеровом обходе, а `getNext(i, x)` - ближайший справа элемент $\leqslant x$, такое умеем за $\langle n, \log n \rangle$ с помощью ДО.
### <span style="color:green">(a)</span> 75. Сумма на пути в дереве за  $\langle \mathcal{O}(n), \mathcal{O}(1) \rangle$.
* Обойдём `DFS`-ом и запомним $l(v)$ - сумма на пути от корня до $v$.
* При запросе на $a$, $b$: `ans = l(a) + l(b) - 2l(lca(a, b))`.
### <span style="color:red">(c)</span> 76. Сумма на пути в меняющемся дереве за $\langle \mathcal{O}(n), \mathcal{O}(\log n) \rangle$.
* Строим Эйлеров обход, и храним `SegTree` для сумм.
* В ребре вверх пишем $w_e$, вниз $-w_e$. При запросе $[in(v), in(u)]$ всё сократится, кроме рёбер на пути.
* Изменение элемента простое базированное для ДО с суммами.
## Другие древесные алгоритмы и структуры

### <span style="color:green">(a)</span> 77. Heavy-Light Decomposition. Построение за $\mathcal{O}(n)$. Одно дерево отрезков.
* Хотим насчитывать целевую функцию на путях дерева. Для бамбука у нас есть дерево отрезков. Для общего случая разобьём дерево на вертикальные пути так, чтоб любой путь раскладывался на $\mathcal{O}(\log n)$ отрезков из этих путей.
* Тяжелое ребро - такое, что размер поддерева сына больше половины размера поддерева вершины.
* Для улучшенной `HLD` во всех вершинах берём самое тяжелое ребро (в сына, у которого максимальный размер поддерева).
* Разбиение дерева за линию, и ещё деревья отрезков на путях тоже за линию.
```cpp
void build_HLD(int v, int p) {
    size[v] = 1;
    heavy_child = -1;
    for (auto u : g[v]) {
        if (u == p) continue;
        build_HLD(u, v);
        size[v] += size[u];
        if heavy_child == -1 || size[u] > size[heavy_child]
            heavy_child = u
    }
    path[v] = heavy_child == -1 ?
        path_n++ : // new path for leaf
        path[heavy_child] // continue path
    pos[v] = len[path[v]]++;
    top[path[v]] = v;
}
```
* Все тяжелые пути можно расположить в одном массиве и сделать на нём одно дерево отрезков
### <span style="color:green">(a)</span> 78. Heavy-Light Decomposition. Вычисление функции на пути, функции поддерева.
* `c := lca(a, b)`, `p := path[a]`
* Теперь пока `p != path[c]`, делаем запрос для `a, top[path[a]]`, затем `p := path[parent[top[path[a]]]]` (не пугайтесь: поднялись от `a` к концу этого тяжелого пути, взобрались на его родителя и уже там мутим всякое).
* Подсчёт функции в худшем случае $\log^2 n$, так как не более двух логарифмов обращений к `ДО`.
* Изменение веса логарифм, так как вершина лежит только в одном `ДО`.
* Можем воспользоваться функцией `isAncestor`, и прыгать из `a` вверх пока не попадём в предка `b`, затем из `b` вверх, пока не попадём в предка `a`. В конце они лежат на одном пути, эту часть нужно тоже учесть. Кстати, получается ищем `LCA` за логарифм с линией прекалка.
* `HLD` умеет всё, что и дерево отрезков, но ещё может поддерживать сложные штуки по типу длины диаметра дерева.
* Ещё можно при построении `HLD` тут же херачить Эйлеров обход, тогда каждый путь - отрезок эйлерова обхода. Отсюда 2 вывода:
    * Можно параллельно применять массовые операции и на путях, и на поддеревьях.
    * Можно хранить одно дерево отрезков на массиве, содержащее сразу все пути.
### <span style="color:blue">(b)</span> 79. Euler-Tour Tree. Операции Link, Cut, IsConnected.
* Хотим поддерживать для графа быстрые операции:
    * `Link(a, b)`: добавить ребро.
    * `Cut(a, b)`: удалить ребро.
    * `IsConnected(a, b)`: проверить связность.
* По сути `Dynamic Connectivity Problem`, но граф - лес.
* `Решение`: храним обычный эйлеров обход дерева (оррёбра, а ещё корень по ребру, вершины по ребру, для каждой вершины одно любое ребро).
* `IsConnected`:
```cpp
bool isConnected(int a, int b) {
    return getRoot(anyEdge[a]) == getRoot(anyEdge[b])
}
```
* `Cut`:
```cpp
void cut(int a, int b) {
    Node* node1 = node[make_pair(a, b)];
    Node* node2 = node[make_pair(b, a)];
    int i = getPosition(node1), j = getPosition(node2);
    if (i > j) swap(i, j);
    Node* root = getRoot(node1), *a, *b, *c;
    Split(root, a, b, i);
    Split(b, b, c, j - i); // (a) (i b) (j c)
    Delete(b, 0), Delete(c, 0) // удалили лишнее ребро
    Merge(a, c) // (a c) (b)
}
```
* `Link`:
```cpp
void link(int a, int b) {
    Node* pa = anyEdge[a], *pb = anyEdge[b];
    Rotate(getRoot(pa), getPosition(pa));
    Rotate(getRoot(pb), getPosition(pb));
    Node *e1 = createEdge(a, b), *e2 = createEdge(b, a);
    Merge(Merge(getRoot(pa), e1), Merge(getRoot(pb), e2));
}
```
### <span style="color:blue">(b)</span> 80. Link-Cut. Описание структуры. Операции Expose, MakeRoot, Link, Cut.
* Умеем брать максимум, менять вес, а ещё `Link/Cut/IsConnected` и не только.
* `Идея`: хранить дерево в виде меняющейся декомпозиции на пути, для каждого пути `rope`, изначально каждая вершина - отдельный путь.
* `GetRoot/GetPos`: для $v$ храним ссылку на её ноду в `rope` её пути, за логарифм можем подняться до корня и параллельно насчитать число вершин левее, то есть позицию. Первым делом для любой операции понадобится:
* `Expose`: меняем декомпозицию дерева на пути. Все вершины от $v$ до корня объединяем в путь. Это ровно $k$ вызовов `split/merge`, где `k` - количество прыжков между путями, докажем амортизированный логарифм.
* `MakeRoot`: после `Expose`, отрезаем часть пути под `v` и делаем `reverse` на её `rope`.
* `Rest ops`: всё остальное теперь выражается через `MakeRoot`:
```cpp
int getMax(int a, int b) {
    MakeRoot(a), MakeRoot(b);
    return GetRoot(a)->max;
}

void link(int a, int b) {
    MakeRoot(a), MakeRoot(b);
    parent[a] = b;
}

void cut(int a, int b) {
    MakeRoot(a), MakeRoot(b);
    split(GetRoot(b), 1);
    parent[a] = -1;
}

bool isConnected(int a, int b) {
    MakeRoot(a), MakeRoot(b);
    return GetRoot(a) == GetRoot(b);
}
```
### <span style="color:blue">(b)</span> 81. Link-Cut. Потенциал. Оценка времени Expose.
* `Th.`: Суммарное время $m$ операций `Expose/MakeRoot` - $\mathcal{O}(n + m\log n)$.
    * `Д-во`: Потенциал $\phi = $ минус "число тяжелых рёбер, покрытых путями". $\phi_0 = 0, \phi \geqslant -n \Rightarrow \sum t_i = \sum a_i + (\phi_0-\phi_m)\leqslant n + \sum a_i$.
    * Оценим $a_i$. Число лёгких рёбер на пути - L, тяжёлых - H
    * `Expose`: $a_i = t_i + \Delta \phi \leqslant t_i - H + L \leqslant k - k + \log n + \log n = \mathcal{O}(\log n)$.
### <span style="color:red">(c)</span> 82. Link-Cut. Оценка времени MakeRoot, Link, Cut.
* `MakeRoot`: `Expose` + `Reverse`, меняет тяжесть только на пути $v\to root$, и лёгких на пути что до что после не более логарифма.
* `Link/Cut`: Аморт.стоимость = реальная + дельта потенциала, реальная стоимость $\mathcal{O}(\log n)$, так как везде вызывается $\mathcal{O}(1)$ операций `Expose/MakeRoot`, а потенциал мог уменьшиться на 1 (добавилось тяжелое ребро) или уменьшиться на 1 (удалили тяжёлое ребро), то есть дельта потенциала $\mathcal{O}(1)$ и асимптотика операций $\mathcal{O}(\log n)$.
### <span style="color:red">(c)</span> 83. MST за линейное время в среднем. Собственно алгоритм.
* Обозначим `F(n, m):= MST` от графа из $n$ вершин и $m$ рёбер. Алгоритм:
    * 3 шага `Борувки`, $n \to n/8$. Время работы $n+m$.
    * Берём случайные $m/2$ рёбер $=:A$. Строим `MST` на `A` вызовом `F(n/8, m/2)`. Полученное `MST =: T`, `A\T` не лежат в `MST(E)`.
    * Переберём рёбра из `B=E\A` и оставим $U\subseteq B$ - те, которые могут улучшить `T` (либо соединить разные компоненты, либо ребро меньше максимума на пути `a,b`). `B\U` не лежат в `MST(E)`.
    * $\text{MST}(E)\subseteq T \cup U$. Сделаем рекурсивный вызов от $F(n/8, |T|+|U|)$.
* Общее время работы: $T \leqslant n/8$. Покажем, что матожидание $|U|\leqslant n/8 - 1$. Тогда суммарное время работы:
    * $F(n, m)\leqslant (n + m) + M(n/8, m/2) + F(n/8, m/2) + F(n/8, n/4)$, где $M(n, m)$ - поиск в офлайне минимумов на $m$ путях в дереве из $n$ вершин, $M(n, m) \leqslant n + m$.
    * Сумма параметров в рекурсивных вызовах $(n+m)/2$ значит всё работает за $\mathcal{O}(n + m)$.
### <span style="color:red">(c)</span> 84. MST за линейное время в среднем. Лемма про количество «небесполезных рёбер».
* Пусть $p$ - вероятность включеня ребра из $E$ в $A$ на втором шаге алгоритма $F$ (у нас 1/2), тогда матожидание $|U| \leqslant (1/p - 1)(n - 1)$.
    * `Д-во`: пусть мы Краскал, который по ходу выполнения при встрече хорошего ребра рандомно решает, класть его в $A$ или $B$. Тогда если Краскал считает, что очередное ребро надо добавить в остов, с вероятностью $p$ добавим ($X$), с $1-p$ скипнем (кладём в $B$) ($Y$). Матожидание числа просмотренных рёбер перед первым событием типа $X$, включая само $X$: $E=1+(1-p)E$, то есть $E=1/p$. Из них первые $1/p - 1$ идут в $U$, а последнее в $T$. В остов добавляется не более $n-1$ рёбер, значит событие $X$ произойдёт не более, чем $n-1$ раз, причем перед каждым $X$ случится $1/p - 1$ событий $Y$. Итого $E[|U|]\leqslant (n-1)(1/p - 1)$.
## Игры

### <span style="color:green">(a)</span> 85. Игры. Ацикличный граф. Решение для симметричной/несимметричной игры.
* `Игра на орграфе`: по вершинам графа перемещается фишка, за ход игрок должен сдвинуть фишку по одному из рёбер. Проигрыш - невозможность сделать ход.
* `Симметричная игра`: ходы одинаковые.
* `Несимметричная игра`: у каждого игрока своё множество рёбер. (Обычно просто вводят пары $\langle v, who \rangle$ - вершина, кто ходит).
* Для симметричных игр для каждой вершины введём $r(G, v)$ - результат игры, `W` - победа, `L` - поражение, `D` - ничья, а так же множества `WIN` = $\{v|r(v)=W\}$, `LOSE` = $\{v|r(v)=L\}$, `DRAW` = $\{v|r(v)=D\}$.
* В несимметриыных играх нужно добавлять, для кого она проигрышна при его первом ходе.
* $W \Leftrightarrow \exists \text{ ход в } L$, $L \Leftrightarrow \text{ все ходы в } W$.
* Решение для ацикличного графа - банальная динамика.
```cpp
int res(int v) {
    int& r = dp[v];
    if (r != -1) return r;
    r = L; // no edges == L
    for (int x : g[v])
        if (res(x) == L) {// looking for an L edge
            r = W;
            break;
        }
    return r;
}
```
### <span style="color:blue">(b)</span> 86. Игры. Граф с циклами. Ретроанализ. Реализация за $\mathcal{O}(E)$. Пример цикла без ничей.
* Что можно пометить `W/L` - помечаем, остальное `D`.
```cpp
queue <-- всё, что W/L по базе.
while (!queue.empty()) {
    v = queue.pop();
    for (auto x : in[v]) { // incoming edges
        if (res[v] == L)
            make_win(x, d[v] + 1);
        else if (++count[x] == deg[x]) // count - число проигрышных рёбер
            make_lose(x, d[v] + 1); 
    }
}
```
* Функции `make_win/make_lose` проверяют, в первый ли раз помечают вершину, и если да, добавляют в очередь, второй параметр - дальность от корня.
* Итог: все помеченные помечены правильно, а непомеченные есть `D` и они ничейные.
    * `Д-во`: из вершины `D` рёбра только в `W` и `D`. Пусть есть в `L`, тогда он была бы помечена `W`. Причём хотя бы одно в `D`. Пусть его нет, тогда все в `W`, пометили бы как `L`. Не хотим проигрывать - не идём в `W` - идём в `D` - вечно ходим по `D`.
### <span style="color:red">(c)</span> 87. Игры. Длина самой короткой игры. Длина самой длинной игры.
* `len(G, v)`: сколько продлится игра, если выигрывающий хочет как можно быстрее выиграть, а проигрывающий затянуть игру.
* После ретроанализа `d[v] = len(G, v)`, так как ретроанализ перебирал вершины в порядке возрастания расстояния, для выигрышной брал наименьшую проигрышную, а для проигрышной наибольшую выигрышную.
* `Садо-мазо версия`: выигрывающий хочет подольше смаковать превосходство, а проигрывающий поскорее проиграть и закончить партию.
    * Запустим ретроанализ, поймём, какие вершины проигрышны, какие выигрышны. 
    * Выкинем неинтересные рёбра, оставим только `L->W` и `W->L`. 
    * Запустим ретроанализ ещё раз, считая по пути длину игры, НО: из выигрышной идём в максимальную проигрышную, из проигрышной - в минимальную выигрышную. 
    * Те, что ничья - бесконечная мука для одного и услада для другого...
### <span style="color:blue">(b)</span> 88. Игры. Ним. Результат игры Ним. Обоснование.
* `Ним`: на столе `n` камней, за ход можно взять любое положительное их число.
* Выигрышная стратегия - взять всё.
### <span style="color:green">(a)</span> 89. Гранди. Функция Гранди, вычисление, связь с выигрышностью.
* `Функция Гранди`: $f[v] = \text{mex}_{v\to x_i}(f[x_i])$.
* $f[v] = 0 \Leftrightarrow v \in $ `LOSE` - элементарная индукция.
### <span style="color:green">(a)</span> 90. Гранди. Прямая сумма. Теорема Гранди про ксор без доказательства. Примеры на тему: Ним, Игра в спички, Скамейки.
* `Прямая сумма` игр $G_1$ и $G_2$ - две игры, можно делать ход в любой из них.
* `Теорема про ксор`: $f[\langle v_1, v_2\rangle] = f[v_1] \oplus f[v_2]$
* `Примеры`:
    * `Ним`: если на столе $m$ кучек, в $i$-й кучке $n_i$ камней, то ф. Гранди для каждой кучки $f[v_i] = n_i$, вся игра - прямая сумма игр на отдельных кучках, тогда $f[v] = \bigoplus n_i$.
    * `Спички`: $n$ стеков из спичек, в $i$-м $a_i$ спичек. За ход можно взять от 1 до $k$ спичек любого стека. $f[v_i] = a_i \% (k+1)$ - доказывается по индукции. Тогда результат игры $\bigoplus (a_i \% (k+1))$.
    * `Скамейки`: по очереди садим человека на свободное место, все соседи которого свободны. $g[n] = \text{mex}\{ g[n-2],g[n-3],g[n-4]\%g[1],g[n-5]\%g[2] \}$ (посадить скраю, посадить в 1 месте от края и т.д.), Посчитав для малых $n$ осознаем, что циклится с периодом 34.
### <span style="color:blue">(b)</span> 91. Гранди. Теорема Гранди про ксор: доказательство.
* `Th.`: $f[\langle v_1, v_2\rangle] = f[v_1] \oplus f[v_2]$. `Д-во`: 
    * `База индукции`: если $f[v_1] = 0$, то вся игра во втором графе и $f[v_1, v_2] = 0 \oplus f[v_2] = f[v_2]$.
    * `Индукционное предположение`: Пусть для всех позиций $(x_1, x_2)$ достижимых из $(v_1, v_2)$ теорема правдива, т е  $f[x_1, x_2] = f[x_1] \oplus f[x_2]$.
    * По определению функции Гранди и прямой суммы игр, множкство достижимых значений $A: = \{ f[v_1] \oplus f[x_{21}], f[v_1] \oplus f[x_{22}], \dots, f[x_{11}]\oplus f[v_2], \dots \}$ (ходы из $v_1$, из $v_2$).
    * $x := f[v_1]$, $y := f[v_2]$, $M:= x\oplus y$. По определению ф. Гранди, из $v_1$ нет хода в $f[x_1] = x$, из $v_2$ нет хода в $f[x_2] = y$, имеем $M\not\in A$.
    * Пусть $k$ - старший бит $M$, то есть в нём $x$ и $y$ различаются. Из $v_1$ переходим во все значения $<x$, из $v_2$ во все значения $<y$, значит комбинируя ходы по свойству $\oplus$ можем составить все значения меньшие $M$. 
    * Получается, $M$ не содержится в $A$, а все числа меньшие $M$ - содержатся. Тогда $mex(A)=M=f[v_1] \oplus f[v_2]$.
### <span style="color:red">(c)</span> 92. Гранди. Максимальное значение функции Гранди.
* Тривиальная оценка: $\mathcal{O}(\max_v \text{deg}_v)$.
* `Lm.`: $\forall G = \langle V, E \rangle, \; \forall v \; f[v] \leqslant \sqrt{2E}$.
    * `Д-во`: Пусть максимум Гранди - $g$, тогда возьмём $k\leqslant g$ и вершину, в которой она достигается (очев, если не достигается, то не может быть бОльших значений ф. Гранди). Из неё должны выходить рёбра во все вершины с $G(v)=0$, $G(v)=1$, $\dots$, $G(v)=k-1$, т.е. $k$ рёбер. То есть из каждой вершины с $G(v)=k$ исходит $k$ рёбер, и сумма рёбер равна $m\geqslant\sum_{i=1}^g i=
\frac{g(g+1)}{2}$, то есть $g^2+g-2m\leqslant0$, то есть $g\leqslant \frac{\sqrt{1+8m}+1}{2}\approx \sqrt{2m}$.